### ELITR-Bench description

This repository contains the dataset associated to ELITR-Bench – a benchmark for the evaluation of long-context LLMs on meeting transcripts. The meeting data used in this benchmark originally comes from the ELITR dataset, which is available [here](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4692).

For more details on this dataset and the experiments we conducted, have a look at [our paper](https://arxiv.org/abs/2403.20262). If you found ELITR-Bench useful and want to refer to our work, please use the following citation:

```bibtex
@article{thonet2024elitrbench,
      title={{ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models}}, 
      author={Thibaut Thonet and Jos Rozen and Laurent Besacier},
      journal={arXiv:2403.20262},
      year={2024},
      url={https://arxiv.org/abs/2403.20262}
}
```

### Repository content

The repository includes two zip archives: `data.zip` and `generated-responses.zip`. The archives are protected by a password to avoid [potential contamination](https://arxiv.org/abs/2310.18018) of LLMs trained on web scrapped data. To unzip the archives, run the commands `unzip data.zip` and `unzip generated-responses.zip` and **indicate '_utter_' as password.**

After these two unzip commands, two folders will be created: `data` and `generated-responses`. The former contains the ELITR-Bench data files with the manually crafted questions, ground-truth answers and metadata. The latter contains the responses generated on ELITR-Bench by different long-context LLMs, with their evaluation. All files are provided in JSON format.

The JSON files are named based on the following template:

- `elitr-bench-{dataset_version}_{split}.json` for the data files;
- `elitr-bench-{dataset_version}_{split}_{inference_mode}_{evaluator}.json` for the generated responses;

where:

- `{dataset_version}` is either the `qa` or `conv` version of ELITR-Bench;
- `{inference_mode}` is either the `st` (single-turn) or `mt` (multi-turn) mode used for inference, i.e., either questions are asked independently or in sequence within the same conversation;
- `{split}` is either the `dev` or `test2` split set;
- `{evaluator}` is either `gpt-4-eval` (indicating that the reported scores are obtained from the GPT-4 evaluator) or `all-eval` (indicating that we report the scores for the 4 different evaluators considered in the "LLM-based evaluation assessment" section of the paper).

### JSON file structure 

Each JSON file is structured as follows:

	{
	  "split": "...",
	  "meetings": [
	    {
	      "id": "...",
	      "questions": [
	        {
	          "id": "...",
	          "question-type": "...",
	          "answer-position": "...",
	          "question": "...",
	          "groundtruth-answer": "...",
	          "generated-responses": [
	            {
	              "model": "...",
	              "generated-response": "...",
	              "gpt-4-eval_score": "..."
	              "prometheus-eval_score": "...",
	              "gold-human-eval_score": "...",
	              "silver-human-eval_score": "..."
	            },
	            ...
	          ]
	        },
	        ...
	      ]
	    },
	    ...
	  ]
	}
	
where:

- `split` is either `dev` or `test2`;
- `meetings` is the list of meetings associated to the set;
- `id` (meeting-level) is the name of the corresponding meeting transcript file from the original ELITR dataset;
- `questions` is the list of questions associated to the meeting;
- `id` (question-level) is a numeric identifier for each question in the meeting;
- `question-type` indicates the type of the questions, among `who`, `what`, `howmany` and `when`;
- `answer-position` indicates the position of the answer in the transcript, among `B` (beginning), `M` (middle), `E` (end) and `S` (several passages across the transcript);
- `question` contains the actual question;
- `groundtruth-answer` contains the manually annotated ground-truth answer to the question;
- `generated-responses` is the list of responses generated by different LLMs to answer the question (this list is only present for the JSON files in the `generated-responses` folder);
- `model` indicates the LLM used to generate the answer;
- `generated-response` contains the actual generated answer;
- `gpt-4-eval_score`, `prometheus-eval_score`, `gold-human-eval_score`, `silver-human-eval_score` indicate the numeric score between 1 and 10 obtained by different evaluators for the generated answer (`prometheus-eval_score`, `gold-human-eval_score`, `silver-human-eval_score` are only reported for the `all-eval` JSON file).

### Funding
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Flag_of_Europe.svg/1200px-Flag_of_Europe.svg.png" width=10% height=10%> 

This is an output of the European Project UTTER (Unified Transcription and Translation for Extended Reality) funded by European Union’s Horizon Europe Research and Innovation programme under grant agreement number 101070631.

For more information please visit https://he-utter.eu/
